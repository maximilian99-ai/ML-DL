{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1faae539e30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 실습하고 있는 파이썬 코드 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드 설정\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train, x_train_shape\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n",
      "\n",
      "y_train, y_train_shape\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 실습을 위한 기본 셋팅 작업 완료 \n",
    "# 훈련 데이터 x_train y_train 을 선언\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])\n",
    "\n",
    "# x_train 와 shape 출력\n",
    "print(\"x_train, x_train_shape\")\n",
    "print(x_train)\n",
    "print(x_train.size()) # or x_trian.shape\n",
    "# x_train 값 x_trian size = 3x1\n",
    "\n",
    "print(\"\")\n",
    "# y_train and y_train shape output\n",
    "print(\"y_train, y_train_shape\")\n",
    "print(y_train)\n",
    "print(y_train.shape)\n",
    "# y_train 값  y_trian size = 3x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가중치 w :  tensor([0.], requires_grad=True)\n",
      "편향 b :  tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치와 편향의 초기화 \n",
    "# 선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일입니다.\n",
    "# 그리고 가장 잘 맞는 직선을 정의하는 것은 바로 W and b 입니다.\n",
    "# 선형희귀의 목표는 가장 잘 맞는 직선을 정의하는 W and b 입니다.\n",
    "\n",
    "# 가중치 0으로 초기화하고 이 값을 출력 편향 b도 0으로 초기화\n",
    "# requires_grad = True -> 학습을 통해 계속. 값이 변경되는 변수임을 의미합니다.\n",
    "w = torch.zeros(1, requires_grad = True)\n",
    "print(\"가중치 w : \", w)\n",
    "\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "print(\"편향 b : \", b)\n",
    "# W 와 b 둘다 0 이므로 현 직선의 방정식 다음과 같습니다.\n",
    "# 현재의 가중치 : y = 0 * x + 0 \n",
    "# 지금 상태에선 x에 어떤 값이 들어가도 가설은 0을 예측하게 됩니다. 즉 아직 적절한 W와 b의 값이 아닙니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설: \n",
      " tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 가설 세우기 \n",
    "# 파이토치 코드 상으로 직선의 방정식에 해당되는 가설을 선언\n",
    "hypothesis = x_train * w + b\n",
    "print(\"가설: \\n\", hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# loss fn 선언 하기 \n",
    "# 평균 제곱 오차를 선언\n",
    "loss = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법 구현 하기 \n",
    "# input w b 가 sgd 의 입력이 됩니다.\n",
    "optimizer = optim.SGD([w, b], lr = 0.01)\n",
    "# SGD -> 경사 하강법의 일종입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W : 0.353 loss : 0.150933\n",
      "Epoch  100/2000 W : 1.746 loss : 0.576683\n",
      "Epoch  200/2000 W : 1.801 loss : 0.453329\n",
      "Epoch  300/2000 W : 1.843 loss : 0.356358\n",
      "Epoch  400/2000 W : 1.877 loss : 0.280130\n",
      "Epoch  500/2000 W : 1.903 loss : 0.220207\n",
      "Epoch  600/2000 W : 1.924 loss : 0.173103\n",
      "Epoch  700/2000 W : 1.940 loss : 0.136075\n",
      "Epoch  800/2000 W : 1.953 loss : 0.106967\n",
      "Epoch  900/2000 W : 1.963 loss : 0.084086\n",
      "Epoch 1000/2000 W : 1.971 loss : 0.066099\n",
      "Epoch 1100/2000 W : 1.977 loss : 0.051960\n",
      "Epoch 1200/2000 W : 1.982 loss : 0.040845\n",
      "Epoch 1300/2000 W : 1.986 loss : 0.032108\n",
      "Epoch 1400/2000 W : 1.989 loss : 0.025240\n",
      "Epoch 1500/2000 W : 1.991 loss : 0.019841\n",
      "Epoch 1600/2000 W : 1.993 loss : 0.015597\n",
      "Epoch 1700/2000 W : 1.995 loss : 0.012260\n",
      "Epoch 1800/2000 W : 1.996 loss : 0.009638\n",
      "Epoch 1900/2000 W : 1.997 loss : 0.007576\n",
      "Epoch 2000/2000 W : 1.997 loss : 0.005956\n"
     ]
    }
   ],
   "source": [
    "# 기울기 0으로 초기화\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# loss fn 미분하여 기울기 계산\n",
    "loss.backward()\n",
    "\n",
    "# w 와 b 값을 업데이트\n",
    "optimizer.step()\n",
    "\n",
    "# 학습을 진행\n",
    "epoch_num = 2000 # 원하는 만큼 경사 하강법을 반복\n",
    "\n",
    "# epoch : 전체 훈련 데이터가 학습에 한 번 사용된 주기를 말합니다.\n",
    "for epoch in range(epoch_num + 1):\n",
    "    \n",
    "    hypothesis = x_train * w + b\n",
    "    \n",
    "    loss = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # loss H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번 마다 print Epoch w b loss\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{} W : {:.3f} loss : {:.6f}\"\n",
    "        .format(epoch, epoch_num, w.item(),  b.item(),  loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is 3\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed() 하는 이유\n",
    "# 결론 : 사용한 프로그램의 결과는 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있습니다.\n",
    "# 이유 : torch.manual_seed()는 난수 발생 순서와 값을 동일하게 보장해주는 특징\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(3)\n",
    "print(\"Random seed is 3\")\n",
    "\n",
    "# range (start , end) -> 출력 범위 지정\n",
    "for i in range(1,3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is 5\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "print(\"Random seed is 5\")\n",
    "\n",
    "for i in range(1,3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "# 자동미분 실습하기\n",
    "import torch\n",
    "\n",
    "# 값이 2인 임의의 스칼라 텐서 w 를 선언 이때 required_grad를 True로 설정합니다. \n",
    "# 이는 이 텐서에 대한 기울기를 저장하겠다는 의미입니다. 뒤에서 보겠지만, 이렇게 하면 w.grad에 w에 대한 미분값이 저장됩니다.\n",
    "# 연산을 기록\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# 2w^2 + 5 -> 8\n",
    "\n",
    "# 수식 정의\n",
    "\n",
    "y = w**2\n",
    "z = 2*y + 5\n",
    "\n",
    "# 이제 해당 수식을 w에 대해서 미분 BackWard()를 호출하면 해당 수식의 w에 대한 기울기 계산\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"수식을 w로 미분한 값 : {}\".format(w.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 다중 선형 회귀 실습\n",
    "# 앞서 배운 x가 1개인 선형 회귀를 단순 선형 이라고 합니다.\n",
    "# 이번 배울것은 다수의 x 로부터 y를 예측하는 다중 선형 회귀\n",
    "\n",
    "# H(x) = w1x1 + w2x2 + w3x3 + b\n",
    "x1_train = torch.FloatTensor([[73],[93],[89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80],[88],[91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75],[93],[90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b를 선언 합니다. 가중치 w도 3개 선언\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b =  torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1 0.294 w2 0.294 w3 0.297 b 0.003 loss 29661.800781\n",
      "Epoch  100/1000 w1 0.674 w2 0.661 w3 0.676 b 0.008 loss 1.563634\n",
      "Epoch  200/1000 w1 0.679 w2 0.655 w3 0.677 b 0.008 loss 1.497603\n",
      "Epoch  300/1000 w1 0.684 w2 0.649 w3 0.677 b 0.008 loss 1.435026\n",
      "Epoch  400/1000 w1 0.689 w2 0.643 w3 0.678 b 0.008 loss 1.375730\n",
      "Epoch  500/1000 w1 0.694 w2 0.638 w3 0.678 b 0.009 loss 1.319503\n",
      "Epoch  600/1000 w1 0.699 w2 0.633 w3 0.679 b 0.009 loss 1.266215\n",
      "Epoch  700/1000 w1 0.704 w2 0.627 w3 0.679 b 0.009 loss 1.215693\n",
      "Epoch  800/1000 w1 0.709 w2 0.622 w3 0.679 b 0.009 loss 1.167821\n",
      "Epoch  900/1000 w1 0.713 w2 0.617 w3 0.680 b 0.009 loss 1.122419\n",
      "Epoch 1000/1000 w1 0.718 w2 0.613 w3 0.680 b 0.009 loss 1.079375\n"
     ]
    }
   ],
   "source": [
    "# 가설 loss fn optim 선언 후 경사 하강법을 1000회 반복\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "epoch_num = 1000\n",
    "\n",
    "for epoch in range(epoch_num + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    # 가설을 선언하는 부분인 \n",
    "    # hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b에서도 x_train의 개수만큼 w와 곱해주도록\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "    \n",
    "    loss = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # loss로 H(x) 개선\n",
    "    optimizer.zero_grad() # 기울기를 0으로 초기화\n",
    "    loss.backward() # loss 함수를 미분하여 기울기 계산\n",
    "    optimizer.step() # w와 b 를 업데이트\n",
    "    \n",
    "    # 100번 마다 로그 출력 \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{} w1 {:.3f} w2 {:.3f} w3 {:.3f} b {:.3f} loss {:.6f}\"\n",
    "            .format(epoch, epoch_num, w1.item(),  w2.item(), w3.item(), b.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25043c5e510>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다중 선형 회귀 클래스 구현\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 생성\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim = 3 , output_dim = 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 정의\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer 선언\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 loss : 31667.597656\n",
      "Epoch  100/2000 loss : 0.225993\n",
      "Epoch  200/2000 loss : 0.223911\n",
      "Epoch  300/2000 loss : 0.221941\n",
      "Epoch  400/2000 loss : 0.220059\n",
      "Epoch  500/2000 loss : 0.218271\n",
      "Epoch  600/2000 loss : 0.216575\n",
      "Epoch  700/2000 loss : 0.214950\n",
      "Epoch  800/2000 loss : 0.213413\n",
      "Epoch  900/2000 loss : 0.211952\n",
      "Epoch 1000/2000 loss : 0.210559\n",
      "Epoch 1100/2000 loss : 0.209230\n",
      "Epoch 1200/2000 loss : 0.207967\n",
      "Epoch 1300/2000 loss : 0.206762\n",
      "Epoch 1400/2000 loss : 0.205618\n",
      "Epoch 1500/2000 loss : 0.204529\n",
      "Epoch 1600/2000 loss : 0.203481\n",
      "Epoch 1700/2000 loss : 0.202486\n",
      "Epoch 1800/2000 loss : 0.201539\n",
      "Epoch 1900/2000 loss : 0.200634\n",
      "Epoch 2000/2000 loss : 0.199770\n"
     ]
    }
   ],
   "source": [
    "# train 생성 \n",
    "# 얼마 만큼 반복할 것 인가 ?!\n",
    "\n",
    "epochs_num =2000\n",
    "for epoch in range(epochs_num + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    # model(x_train) == model.forward(x_train)\n",
    "    \n",
    "    loss = F.mse_loss(prediction, y_train) # 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "    \n",
    "    # loss 개선\n",
    "    optimizer.zero_grad() # 기울기를 0으로 초기화\n",
    "    loss.backward() # loss 함수를 미분하여 기울기 계산\n",
    "    optimizer.step() # w와 b 를 업데이트\n",
    "    \n",
    "    # 100번 마다 로그 출력 \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{} loss : {:.6f}\".format(epoch, epochs_num, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 tensor([[73., 82., 72.]])일 때의 예측값:  tensor([[150.4079]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 훈련 여부 확인하기 \n",
    "# 임의의 입력 [73, 82, 72]\n",
    "new_var = torch.FloatTensor([[73, 82, 72]])\n",
    "pred_y = model(new_var)\n",
    "print(f\"훈련 후 입력이 {new_var}일 때의 예측값: \", pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/200 Batch 1/3 loss : 46978.789062\n",
      "Epoch    0/200 Batch 2/3 loss : 6497.784180\n",
      "Epoch    0/200 Batch 3/3 loss : 5174.395996\n",
      "Epoch   10/200 Batch 1/3 loss : 20.487284\n",
      "Epoch   10/200 Batch 2/3 loss : 7.705010\n",
      "Epoch   10/200 Batch 3/3 loss : 7.212080\n",
      "Epoch   20/200 Batch 1/3 loss : 17.549288\n",
      "Epoch   20/200 Batch 2/3 loss : 7.824885\n",
      "Epoch   20/200 Batch 3/3 loss : 24.961105\n",
      "Epoch   30/200 Batch 1/3 loss : 22.525852\n",
      "Epoch   30/200 Batch 2/3 loss : 14.071042\n",
      "Epoch   30/200 Batch 3/3 loss : 10.301792\n",
      "Epoch   40/200 Batch 1/3 loss : 6.263861\n",
      "Epoch   40/200 Batch 2/3 loss : 12.951877\n",
      "Epoch   40/200 Batch 3/3 loss : 26.012863\n",
      "Epoch   50/200 Batch 1/3 loss : 29.166300\n",
      "Epoch   50/200 Batch 2/3 loss : 14.010695\n",
      "Epoch   50/200 Batch 3/3 loss : 1.219263\n",
      "Epoch   60/200 Batch 1/3 loss : 6.347510\n",
      "Epoch   60/200 Batch 2/3 loss : 12.247962\n",
      "Epoch   60/200 Batch 3/3 loss : 24.989016\n",
      "Epoch   70/200 Batch 1/3 loss : 0.935088\n",
      "Epoch   70/200 Batch 2/3 loss : 27.385311\n",
      "Epoch   70/200 Batch 3/3 loss : 20.279123\n",
      "Epoch   80/200 Batch 1/3 loss : 7.437129\n",
      "Epoch   80/200 Batch 2/3 loss : 11.417286\n",
      "Epoch   80/200 Batch 3/3 loss : 20.715305\n",
      "Epoch   90/200 Batch 1/3 loss : 16.228043\n",
      "Epoch   90/200 Batch 2/3 loss : 7.943784\n",
      "Epoch   90/200 Batch 3/3 loss : 14.217880\n",
      "Epoch  100/200 Batch 1/3 loss : 8.686045\n",
      "Epoch  100/200 Batch 2/3 loss : 8.259483\n",
      "Epoch  100/200 Batch 3/3 loss : 30.497574\n",
      "Epoch  110/200 Batch 1/3 loss : 13.453944\n",
      "Epoch  110/200 Batch 2/3 loss : 1.228231\n",
      "Epoch  110/200 Batch 3/3 loss : 30.029516\n",
      "Epoch  120/200 Batch 1/3 loss : 13.239824\n",
      "Epoch  120/200 Batch 2/3 loss : 20.556040\n",
      "Epoch  120/200 Batch 3/3 loss : 8.727528\n",
      "Epoch  130/200 Batch 1/3 loss : 11.462080\n",
      "Epoch  130/200 Batch 2/3 loss : 16.164177\n",
      "Epoch  130/200 Batch 3/3 loss : 10.231584\n",
      "Epoch  140/200 Batch 1/3 loss : 8.183414\n",
      "Epoch  140/200 Batch 2/3 loss : 7.312720\n",
      "Epoch  140/200 Batch 3/3 loss : 28.225088\n",
      "Epoch  150/200 Batch 1/3 loss : 4.191692\n",
      "Epoch  150/200 Batch 2/3 loss : 15.761783\n",
      "Epoch  150/200 Batch 3/3 loss : 13.685800\n",
      "Epoch  160/200 Batch 1/3 loss : 12.901405\n",
      "Epoch  160/200 Batch 2/3 loss : 6.517171\n",
      "Epoch  160/200 Batch 3/3 loss : 13.143169\n",
      "Epoch  170/200 Batch 1/3 loss : 11.106392\n",
      "Epoch  170/200 Batch 2/3 loss : 10.358293\n",
      "Epoch  170/200 Batch 3/3 loss : 5.937093\n",
      "Epoch  180/200 Batch 1/3 loss : 22.356493\n",
      "Epoch  180/200 Batch 2/3 loss : 11.431027\n",
      "Epoch  180/200 Batch 3/3 loss : 1.302418\n",
      "Epoch  190/200 Batch 1/3 loss : 10.271872\n",
      "Epoch  190/200 Batch 2/3 loss : 5.642936\n",
      "Epoch  190/200 Batch 3/3 loss : 16.062317\n",
      "Epoch  200/200 Batch 1/3 loss : 9.474249\n",
      "Epoch  200/200 Batch 2/3 loss : 6.059466\n",
      "Epoch  200/200 Batch 3/3 loss : 14.952425\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서 데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "\n",
    "# TensorDataset은 기본적으로 텐서를 입력으로 받습니다. 텐서 형태로 데이터를 정의합니다.\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# TensorDataset의 입력으로 사용하고 dataset으로 저장합니다.\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "\"\"\"\n",
    "데이터로더는 기본적으로 2개의 인자를 입력받는다. 하나는 데이터셋, 미니 배치의 크기입니다. \n",
    "이때 미니 배치의 크기는 통상적으로 2의 배수를 사용합니다. (ex) 64, 128, 256...) 그리고 추가적으로 많이 사용되는 인자로 shuffle이 있습니다. \n",
    "shuffle=True를 선택하면 Epoch마다 데이터셋을 섞어서 데이터가 학습되는 순서를 바꿉니다.\n",
    "\n",
    "사람도 같은 문제지를 계속 풀면 어느 순간 문제의 순서에 익숙해질 수 있습니다. \n",
    "예를 들어 어떤 문제지의 12번 문제를 풀면서, '13번 문제가 뭔지는 기억은 안 나지만 어제 풀었던 기억으로 정답은 5번이었던 것 같은데' \n",
    "하면서 문제 자체보단 순서에 익숙해질 수 있다는 것입니다. 그럴 때 문제지를 풀 때마다 문제 순서를 랜덤으로 바꾸면 도움이 될 겁니다. \n",
    "마찬가지로 모델이 데이터셋의 순서에 익숙해지는 것을 방지하여 학습할 때는 이 옵션을 True를 주는 것을 권장합니다.\n",
    "\"\"\"\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)\n",
    "\n",
    "# 이제 모델과 옵티마이저 설계\n",
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
    "\n",
    "epochs_nb = 200\n",
    "for epoch in range(epochs_nb + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        \n",
    "        #print(batch_idx)\n",
    "        #print(samples)\n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        loss = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # cost H(x) 계산\n",
    "        optimizer.zero_grad() # 기울기를 0으로 초기화\n",
    "        loss.backward() # loss 함수를 미분하여 기울기 계산\n",
    "        optimizer.step() # w와 b 를 업데이트\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            \n",
    "            print(\"Epoch {:4d}/{} Batch {}/{} loss : {:.6f}\".format(epoch, epochs_nb, batch_idx+1, len(dataloader), \n",
    "            loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73 80 75일 때 예측값 :  tensor([[154.5247]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 모델의 입력으로 임의의 값을 넣어 예측값을 확인\n",
    "\n",
    "# 임의의 입력 값\n",
    "test_val = torch.FloatTensor([[73, 80, 75]])\n",
    "\n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(test_val)\n",
    "print(\"훈련 후 입력이 73 80 75일 때 예측값 : \", pred_y)\n",
    "\n",
    "# 정답지 : 73 80 75 -> y 값이 152"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
